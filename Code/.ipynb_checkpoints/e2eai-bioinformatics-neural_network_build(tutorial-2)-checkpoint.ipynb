{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Copyright (c) 2021 F.J. Greco\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinformatics Modeling\n",
    "\n",
    "## Neural Network Builder - Tutorial 2 \n",
    "\n",
    "### Contact: fjgreco@us.ibm.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following if in CP4D"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from project_lib import Project\n",
    "project = Project.access()\n",
    "storage_credentials = project.get_storage_metadata()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ls /project_data/data_asset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('/project_data/data_asset/<neural_network.py>') as py:\n",
    "    new_nnfile=py.read()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%writefile  /project_data/data_asset/<new_neural_network.py>\n",
    "%load /project_data/data_asset/<neural_network.py>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline editing of code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing new_neural_network.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile new_neural_network.py\n",
    "#%load /project_data/data_asset/new_neural_network8.py\n",
    "import argparse\n",
    "#import input_data\n",
    "import os\n",
    "import sys\n",
    "import tensorflow \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten,LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def main():\n",
    "    \n",
    "    import os\n",
    "    \"\"\"\n",
    "    cmdstring = 'pip install matplotlib'\n",
    "    os.system(cmdstring)\n",
    "    import matplotlib.pyplot as plt\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # environment variable when name starts with $\n",
    "    parser.add_argument('--data_dir', type=str, default='$DATA_DIR',help='Directory with data')\n",
    "    parser.add_argument('--result_dir', type=str, default='$RESULT_DIR',help='Directory with results')\n",
    "    parser.add_argument('--sequences_file', type=str,default='sequences.txt',help='File name for sequences')\n",
    "    parser.add_argument('--labels_file', type=str,default='labels.txt',help='File name for labels')\n",
    "    parser.add_argument('--model_name', type=str,default='bioinformatics_model',help='neural model name')\n",
    "    parser.add_argument('--lstm',type=bool,default=True,help='Include LSTM')\n",
    "    parser.add_argument('--epochs',type=int,default=10,help='Number of epochs')\n",
    "    parser.add_argument('--lr',type=float,default=0.01,help='Learning rate')\n",
    "    parser.add_argument(\"--feature_shape\",type=int,default=50,help='Feature shape')\n",
    "\n",
    "\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    print (FLAGS.result_dir)\n",
    "\n",
    "    if (FLAGS.result_dir[0] == '$'):\n",
    "        RESULT_DIR = os.environ[FLAGS.result_dir[1:]]\n",
    "    else:\n",
    "        RESULT_DIR = FLAGS.result_dir\n",
    "        os.environ['RESULT_DIR']=FLAGS.result_dir\n",
    "\n",
    "    #model_path = os.path.join(RESULT_DIR, 'model')\n",
    "    #print(model_path)\n",
    "\n",
    "    if (FLAGS.data_dir[0] == '$'):\n",
    "        DATA_DIR = os.environ[FLAGS.data_dir[1:]]\n",
    "    else:\n",
    "        DATA_DIR = FLAGS.data_dir\n",
    "        os.environ['DATA_DIR']=FLAGS.data_dir\n",
    "        \n",
    "    output_model_folder = os.environ[\"RESULT_DIR\"]\n",
    "\n",
    "    print(\"output model folder: \",output_model_folder)\n",
    "    \n",
    "    model_name=FLAGS.model_name\n",
    "    \n",
    "    history_filename  = model_name+\"_history.p\"\n",
    "    print(\"history_filename: \",history_filename)\n",
    "    \n",
    "    cm_filename  = model_name+\"_cm.p\"\n",
    "    print(\"cm_filename: \",cm_filename)\n",
    "    \n",
    "    h5_filename  = model_name+\".h5\"\n",
    "    print(\"h5_filename: \",h5_filename)\n",
    "    \n",
    "    tar_filename = model_name+\".tgz\"\n",
    "    print(\"tar_filename: \",tar_filename)\n",
    "    \n",
    "    model_weights = model_name + \"_weights.h5\"\n",
    "    print(\"model_weights: \", model_weights)\n",
    "    \n",
    "    serialized_model = model_name + \".json\"\n",
    "    print(\"serialized_model: \", serialized_model)\n",
    "   \n",
    "    \n",
    "    scoring_log = model_name + \"_scoring.txt\"\n",
    "    \n",
    "    loss_graph_pdf= model_name + \"_loss.pdf\"\n",
    "    loss_graph_png = model_name + \"_loss.png\"\n",
    "    print(\"loss_graph:\",loss_graph_png)\n",
    "    \n",
    "    accuracy_graph_pdf = model_name + \"_accuracy.pdf\"\n",
    "    accuracy_graph_png = model_name + \"_accuracy.png\"\n",
    "    print(\"accuracy_graph:\",accuracy_graph_png)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Set training hyperparameters\n",
    "    #\n",
    "    \n",
    "    epochs = FLAGS.epochs\n",
    "    #epochs = 50\n",
    "    lr     = FLAGS.lr\n",
    "    #lr=  0.01\n",
    "    lstm   = FLAGS.lstm\n",
    "    feature_shape = FLAGS.feature_shape\n",
    "    \n",
    "    #\n",
    "    # Print hyperparameters to stdout\n",
    "    #\n",
    "    \n",
    "    print('\\n')\n",
    "    print(\"Number of epochs: \", epochs )\n",
    "    print(\"Learning Rate:    \", lr)\n",
    "    print(\"Include LSTM:     \", lstm )\n",
    "    print(\"Feature Shape:    \", feature_shape )\n",
    "   \n",
    "    \n",
    "\n",
    "    # Add data dir to file path\n",
    "    sequences_file = os.path.join(DATA_DIR, FLAGS.sequences_file)\n",
    "    \n",
    "    labels_file = os.path.join(DATA_DIR, FLAGS.labels_file)\n",
    "    \n",
    "    #\n",
    "    # One-hot encode feature data\n",
    "    #\n",
    "    \n",
    "    with open(sequences_file,'r') as file: \n",
    "        raw_sequences=file.read()\n",
    "\n",
    "    sequences=raw_sequences.split('\\n')\n",
    "\n",
    "    sequences = list(filter(None, sequences))  # Removes empty sequences.\n",
    "\n",
    "    integer_encoder = LabelEncoder() \n",
    "\n",
    "    one_hot_encoder = OneHotEncoder(categories='auto')  \n",
    "    \n",
    "    input_features = []\n",
    "\n",
    "    for sequence in sequences:\n",
    "        integer_encoded = integer_encoder.fit_transform(list(sequence))\n",
    "        integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "        one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "        input_features.append(one_hot_encoded.toarray())\n",
    "\n",
    "\n",
    "    np.set_printoptions(threshold=40)\n",
    "    input_features = np.stack(input_features)\n",
    "   \n",
    "    print(\"Sequence 1\\n-----------------------\")\n",
    "    print('DNA Sequence #1:\\n',sequences[0][:10],'...',sequences[0][-10:])\n",
    "    print('One hot encoding of Sequence #1:\\n',input_features[0].T)\n",
    "\n",
    "    #\n",
    "    # One-hot encode labels\n",
    "    #\n",
    "    with open(labels_file,'r') as file: \n",
    "        raw_labels=file.read()\n",
    "\n",
    "    labels=raw_labels.split('\\n')\n",
    "\n",
    "    labels = list(filter(None, labels))  # This removes empty sequences.\n",
    "\n",
    "    one_hot_encoder = OneHotEncoder(categories='auto')\n",
    "    labels = np.array(labels).reshape(-1, 1)\n",
    "    input_labels = one_hot_encoder.fit_transform(labels).toarray()\n",
    "\n",
    "    print('Labels:\\n',labels.T)\n",
    "    print('One-hot encoded labels:\\n',input_labels.T)\n",
    "\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        input_features, input_labels, test_size=0.25, random_state=42)\n",
    "        \n",
    "            \n",
    "    #\n",
    "    # Define the neural network model\n",
    "    #  \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=12, \n",
    "                 input_shape=(train_features.shape[1], 4)))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    if lstm == True:\n",
    "            model.add(LSTM(feature_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    opt = Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "        metrics=['binary_accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    #\n",
    "    # Train the model\n",
    "    #\n",
    "    \n",
    "    history = model.fit(train_features, train_labels, \n",
    "            epochs=75,  verbose=0, validation_split=0.25)\n",
    "    \n",
    "    import pickle\n",
    "    with open(history_filename, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    cmdstring0 = 'cp ' + history_filename + ' '+  output_model_folder\n",
    "    os.system(cmdstring0)\n",
    "    \n",
    "    #\n",
    "    # Save model to the results storage\n",
    "    #\n",
    "    \n",
    "    model.save( h5_filename ) \n",
    "    cmdstring1 = 'cp ' + h5_filename + ' '+  output_model_folder\n",
    "    os.system(cmdstring1)\n",
    "\n",
    "    cmdstring2 = 'tar -zcvf ' + tar_filename + ' ' + h5_filename\n",
    "    os.system(cmdstring2)\n",
    "    \n",
    "    cmdstring22 = 'cp ' + tar_filename + ' '+  output_model_folder\n",
    "    os.system(cmdstring22)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Save the model definition to the results storage\n",
    "    #\n",
    "    model_json = model.to_json()\n",
    "    with open(serialized_model, \"w\") as json_file:\n",
    "        json_file.write(model_json)     \n",
    " \n",
    "    cmdstring3 = 'cp ' + serialized_model + ' '+  output_model_folder\n",
    "    os.system(cmdstring3)\n",
    "\n",
    "    #\n",
    "    # Save  trained model weights to the results storage\n",
    "    #\n",
    "    model.save_weights(model_weights)\n",
    "    cmdstring4 = 'cp ' + model_weights + ' '+  output_model_folder\n",
    "    os.system(cmdstring4)\n",
    "    \n",
    "    \n",
    "    ## Produce and save a confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    #import itertools\n",
    "\n",
    "    predicted_labels = model.predict(np.stack(test_features))\n",
    "    cm = confusion_matrix(np.argmax(test_labels, axis=1), \n",
    "                          np.argmax(predicted_labels, axis=1))\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    with open(cm_filename, 'wb') as file_pi:\n",
    "        pickle.dump(cm, file_pi)\n",
    "    \n",
    "    cmdstringX = 'cp ' + cm_filename + ' '+  output_model_folder\n",
    "    os.system(cmdstringX)\n",
    " \n",
    "    scores = model.evaluate(test_features, test_labels, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code locally (either on your desktop or Watson Studio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 432\r\n",
      "drwxr-xr-x  15 fjgreco  staff     480 Jun  6 21:26 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x@ 14 fjgreco  staff     448 Jun  6 21:21 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 fjgreco  staff    6148 Jun  6 21:19 .DS_Store\r\n",
      "drwxr-xr-x   3 fjgreco  staff      96 Jun  6 21:25 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   5 fjgreco  staff     160 Jun  6 21:25 \u001b[34mDATA_DIR\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 fjgreco  staff    8577 Jun  2 21:10 ICOS.py\r\n",
      "drwxr-xr-x   3 fjgreco  staff      96 Jun  6 21:25 \u001b[34m__pycache__\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 fjgreco  staff  100044 Jun  6 19:55 e2eai-bioinformatics-analysis(tutorial-4).ipynb\r\n",
      "-rw-r--r--   1 fjgreco  staff   16326 Jun  6 21:25 e2eai-bioinformatics-assay(tutorial-1).ipynb\r\n",
      "-rw-r--r--   1 fjgreco  staff   26032 Jun  6 21:15 e2eai-bioinformatics-neural_network_build(tutorial-2).ipynb\r\n",
      "-rw-r--r--@  1 fjgreco  staff    1068 Jun  6 15:05 e2eai_credentials.json\r\n",
      "-rw-r--r--   1 fjgreco  staff    7898 Jun  6 21:26 new_neural_network.py\r\n",
      "drwxr-xr-x   4 fjgreco  staff     128 Jun  6 18:54 \u001b[34mtf_model_v8T\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 fjgreco  staff    5650 Jun  6 20:55 tf_model_v8T.zip\r\n",
      "-rw-r--r--   1 fjgreco  staff   31647 Jun  6 19:56 wml-v4-bioinformatics-train(tutorial-3).ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: DATA_DIR: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir DATA_DIR  # Local training directory copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir RESULT_DIR # Local results directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy a subset of training data to TEST DATA DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir SUBSET_DATA_DIR # Local directory with subset of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rec=200\n",
    "rec_count=0\n",
    "with open('DATA_DIR/assay_data_full.lbl','r') as fi, open('SUBSET_DATA_DIR/assay_data_test.lbl', 'w') as fo:\n",
    "    for line in fi:\n",
    "        if rec_count==max_rec:\n",
    "            break\n",
    "        else:\n",
    "            fo.write(line)\n",
    "            rec_count += 1\n",
    "rec_count=0           \n",
    "with open('DATA_DIR/assay_data_full.seq','r') as fi, open('SUBSET_DATA_DIR/assay_data_test.seq', 'w') as fo:\n",
    "    for line in fi:\n",
    "        if rec_count==max_rec:\n",
    "            break\n",
    "        else:\n",
    "            fo.write(line)\n",
    "            rec_count += 1\n",
    "                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assay_data_test.lbl assay_data_test.seq\r\n"
     ]
    }
   ],
   "source": [
    "!ls SUBSET_DATA_DIR"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If running in CP4D\n",
    "cp /project_data/data_asset/DATADIR/* TEST_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set environment variables that will be picked up by the payload program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['RESULT_DIR']='RESULT_DIR'\n",
    "os.environ['DATA_DIR']='SUBSET_DATA_DIR'   #Set DATA_DIR to TEST_DATA_DIR for local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBSET_DATA_DIR\r\n"
     ]
    }
   ],
   "source": [
    "!echo $DATA_DIR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT_DIR\r\n"
     ]
    }
   ],
   "source": [
    "!echo $RESULT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assay_data_full.csv assay_data_full.lbl assay_data_full.seq\r\n"
     ]
    }
   ],
   "source": [
    "!ls DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>Set parameters passed to the payload: </font>\n",
    "\n",
    "<strong>labels_file</strong>and <strong>sequences_file</strong> must reference files in the DATA_DIR directory\n",
    "\n",
    "<strong>feature_shape</strong> must patch the value coded in the payload\n",
    "\n",
    "<strong>lstm</strong> determines whether the neural  network should include a LSTM layer\n",
    "\n",
    "<strong>epochs</strong> determines the number of passes thru the training data\n",
    "\n",
    "<strong>lr</strong> can be passed to set the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$RESULT_DIR\n",
      "output model folder:  RESULT_DIR\n",
      "history_filename:  bioinformatics_model_history.p\n",
      "cm_filename:  bioinformatics_model_cm.p\n",
      "h5_filename:  bioinformatics_model.h5\n",
      "tar_filename:  bioinformatics_model.tgz\n",
      "model_weights:  bioinformatics_model_weights.h5\n",
      "serialized_model:  bioinformatics_model.json\n",
      "loss_graph: bioinformatics_model_loss.png\n",
      "accuracy_graph: bioinformatics_model_accuracy.png\n",
      "\n",
      "\n",
      "Number of epochs:  10\n",
      "Learning Rate:     0.01\n",
      "Include LSTM:      True\n",
      "Feature Shape:     50\n",
      "Sequence 1\n",
      "-----------------------\n",
      "DNA Sequence #1:\n",
      " CGAGCCAATC ... TTGCGAGGAA\n",
      "One hot encoding of Sequence #1:\n",
      " [[0. 0. 1. ... 0. 1. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Labels:\n",
      " [['0' '0' '0' ... '0' '0' '1']]\n",
      "One-hot encoded labels:\n",
      " [[1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "2021-06-06 21:27:45.020972: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f9a0127eb80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-06-06 21:27:45.021009: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 39, 32)            1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 9, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                16600     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 19,018\n",
      "Trainable params: 19,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "a bioinformatics_model.h5\n",
      "binary_accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "!python3 new_neural_network.py --sequences_file assay_data_test.seq --labels_file assay_data_test.lbl --feature_shape=50 --epochs=10 --lstm=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create zip payload file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp new_neural_network.py neural_network_v8T.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: tf_model_v8T: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir tf_model_v8T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp neural_network_v8T.py tf_model_v8T/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: tf_model_v8T/ (stored 0%)\r\n",
      "updating: tf_model_v8T/neural_network_v8T.py (deflated 69%)\r\n",
      "updating: tf_model_v8T/.ipynb_checkpoints/ (stored 0%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r tf_model_v8T.zip tf_model_v8T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1640\r\n",
      "drwxr-xr-x  24 fjgreco  staff     768 Jun  6 21:28 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x@ 14 fjgreco  staff     448 Jun  6 21:21 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 fjgreco  staff    6148 Jun  6 21:19 .DS_Store\r\n",
      "drwxr-xr-x   4 fjgreco  staff     128 Jun  6 21:27 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   5 fjgreco  staff     160 Jun  6 21:25 \u001b[34mDATA_DIR\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 fjgreco  staff    8577 Jun  2 21:10 ICOS.py\r\n",
      "drwxr-xr-x   8 fjgreco  staff     256 Jun  6 21:27 \u001b[34mRESULT_DIR\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   4 fjgreco  staff     128 Jun  6 21:27 \u001b[34mSUBSET_DATA_DIR\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 fjgreco  staff      96 Jun  6 21:25 \u001b[34m__pycache__\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 fjgreco  staff  270440 Jun  6 21:27 bioinformatics_model.h5\r\n",
      "-rw-r--r--   1 fjgreco  staff    2703 Jun  6 21:27 bioinformatics_model.json\r\n",
      "-rw-r--r--   1 fjgreco  staff  218334 Jun  6 21:27 bioinformatics_model.tgz\r\n",
      "-rw-r--r--   1 fjgreco  staff     189 Jun  6 21:27 bioinformatics_model_cm.p\r\n",
      "-rw-r--r--   1 fjgreco  staff    8229 Jun  6 21:27 bioinformatics_model_history.p\r\n",
      "-rw-r--r--   1 fjgreco  staff   95616 Jun  6 21:27 bioinformatics_model_weights.h5\r\n",
      "-rw-r--r--   1 fjgreco  staff  100044 Jun  6 19:55 e2eai-bioinformatics-analysis(tutorial-4).ipynb\r\n",
      "-rw-r--r--   1 fjgreco  staff   16326 Jun  6 21:25 e2eai-bioinformatics-assay(tutorial-1).ipynb\r\n",
      "-rw-r--r--   1 fjgreco  staff   24437 Jun  6 21:27 e2eai-bioinformatics-neural_network_build(tutorial-2).ipynb\r\n",
      "-rw-r--r--@  1 fjgreco  staff    1068 Jun  6 15:05 e2eai_credentials.json\r\n",
      "-rw-r--r--   1 fjgreco  staff    7898 Jun  6 21:28 neural_network_v8T.py\r\n",
      "-rw-r--r--   1 fjgreco  staff    7898 Jun  6 21:26 new_neural_network.py\r\n",
      "drwxr-xr-x   4 fjgreco  staff     128 Jun  6 18:54 \u001b[34mtf_model_v8T\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 fjgreco  staff    5650 Jun  6 21:28 tf_model_v8T.zip\r\n",
      "-rw-r--r--   1 fjgreco  staff   31647 Jun  6 19:56 wml-v4-bioinformatics-train(tutorial-3).ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=green>Proceed to running wml-v4--bioinformatics-neural_network_train(tutorial).ipynb...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
